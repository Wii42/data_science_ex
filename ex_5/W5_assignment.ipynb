{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>Week 5 - Assignment</center></h2>\n",
    "<h3><center>Programming for Data Science 2024</center></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises for the topics covered in the fifth lecture.\n",
    "\n",
    "The exercise will be marked as passed if you get **at least 10/15** points.\n",
    "\n",
    "Exercises must be handed in via **ILIAS** (Homework assignments). Deliver your submission as a compressed file (zip) containing one .py or .ipynb file with all exercises. The name of both the .zip and the .py/.ipynb file **must** be *SurnameName* of the two members of the group. Example: Riccardo Cusinato + Athina Tzovara = *CusinatoRiccardo_TzovaraAthina.zip* .\n",
    "\n",
    "It's important to use comments to explain your code and show that you're able to take ownership of the exercises and discuss them.\n",
    "\n",
    "You are not expected to collaborate outside of the group on exercises and submitting other groups’ code as your own will result in 0 points.\n",
    "\n",
    "For questions contact: *riccardo.cusinato@unibe.ch* with the subject: *Programming for Data Science 2024*.\n",
    "\n",
    "**Deadline: 14:00, March 28, 2024.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:left;\">Exercise 1 - Fitbit dataset<span style=\"float: right\">3 points</span></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with three datasets - 'activity.csv', 'calories.csv', and 'last_participant.csv', which contains activity tracker data from https://www.kaggle.com/datasets/arashnic/fitbit\n",
    "\n",
    "If you are unable to do this exercise, you can load the dataset 'combined_solution.csv' for the next exercise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Data preparation** (*1 point*)\n",
    "\n",
    "    - Load the two datasets 'activity.csv' and 'calories.csv'.\n",
    "    - Use pd.to_datetime to standardize the ActivityDate columns (https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T00:11:46.794960Z",
     "start_time": "2024-03-28T00:11:46.775390Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lukik\\AppData\\Local\\Temp\\ipykernel_18400\\3251334.py:5: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['ActivityDate'] = pd.to_datetime(df[\n",
      "C:\\Users\\lukik\\AppData\\Local\\Temp\\ipykernel_18400\\3251334.py:5: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['ActivityDate'] = pd.to_datetime(df[\n"
     ]
    },
    {
     "data": {
      "text/plain": "             ID ActivityDate  Calories\n0    1624580081   2016-04-12    1432.0\n1    1624580081   2016-04-13    1411.0\n2    1624580081   2016-04-14    1572.0\n3    1624580081   2016-04-15    1344.0\n4    1624580081   2016-04-16       NaN\n..          ...          ...       ...\n904  8792009665   2016-05-06    1688.0\n905  8792009665   2016-05-07    1688.0\n906  8792009665   2016-05-08    1688.0\n907  8792009665   2016-05-09    1688.0\n908  8792009665   2016-05-10       NaN\n\n[909 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>ActivityDate</th>\n      <th>Calories</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1624580081</td>\n      <td>2016-04-12</td>\n      <td>1432.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1624580081</td>\n      <td>2016-04-13</td>\n      <td>1411.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1624580081</td>\n      <td>2016-04-14</td>\n      <td>1572.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1624580081</td>\n      <td>2016-04-15</td>\n      <td>1344.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1624580081</td>\n      <td>2016-04-16</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>904</th>\n      <td>8792009665</td>\n      <td>2016-05-06</td>\n      <td>1688.0</td>\n    </tr>\n    <tr>\n      <th>905</th>\n      <td>8792009665</td>\n      <td>2016-05-07</td>\n      <td>1688.0</td>\n    </tr>\n    <tr>\n      <th>906</th>\n      <td>8792009665</td>\n      <td>2016-05-08</td>\n      <td>1688.0</td>\n    </tr>\n    <tr>\n      <th>907</th>\n      <td>8792009665</td>\n      <td>2016-05-09</td>\n      <td>1688.0</td>\n    </tr>\n    <tr>\n      <th>908</th>\n      <td>8792009665</td>\n      <td>2016-05-10</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>909 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def standardize_activity_date(df: pd.DataFrame):\n",
    "    df['ActivityDate'] = pd.to_datetime(df[\n",
    "                                            'ActivityDate'])  # no format specified, since multiple formats exist in the data and the inferred format seems to be correct everywhere\n",
    "\n",
    "\n",
    "activity_df: pd.DataFrame = pd.read_csv('./data/activity.csv')\n",
    "calories_df: pd.DataFrame = pd.read_csv('./data/calories.csv')\n",
    "standardize_activity_date(activity_df)\n",
    "standardize_activity_date(calories_df)\n",
    "calories_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Merging** (*1 point*)\n",
    "\n",
    "    - Consider what information is shared between the two datasets and merge them. Keep in mind that the order of rows is not the same in both datasets!\n",
    "    - Print out the mean \"TotalSteps\" of the merged DataFrame at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T00:11:46.843929Z",
     "start_time": "2024-03-28T00:11:46.835844Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean TotalSteps: 7786.438763376932\n"
     ]
    }
   ],
   "source": [
    "combined_df: pd.DataFrame = pd.merge(activity_df, calories_df, on=['ID',\n",
    "                                                                   'ActivityDate'])  # id and activityDate is in both datasets and together are a unique key for each row, since both id and on their own are not unique, there are multiple entries with the same date.\n",
    "\n",
    "print(\"Mean TotalSteps: \" + str(combined_df['TotalSteps'].mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Concatenation** (*1 point*)\n",
    "\n",
    "    - The data of one additional participant exists in 'last_participant.csv'. Load this dataset and concatenate it with the merged dataset generated above\n",
    "    - Print out the mean \"TotalSteps\" again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T00:11:46.854668Z",
     "start_time": "2024-03-28T00:11:46.845005Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean TotalSteps last participant included: 7879.460599334073\n"
     ]
    }
   ],
   "source": [
    "last_participant_df: pd.DataFrame = pd.read_csv('./data/last_participant.csv')\n",
    "combined_df = pd.concat([combined_df, last_participant_df])\n",
    "\n",
    "print(\"Mean TotalSteps last participant included: \" + str(combined_df['TotalSteps'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:left;\">Exercise 2 - Working with missing data<span style=\"float: right\">5 points</span></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our dataset, some values are missing from the 'TotalSteps' and 'Calories' columns.\n",
    "\n",
    "We can try to approximate these missing values with the data we got. \n",
    "\n",
    "You can load the dataset 'combined_solution.csv' if you were unable to complete the previous exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Filling in missing values** (*3 points*)\n",
    "\n",
    "    - Calculate the mean steps per calory burnt and mean calories burnt per step, by averaging across all observations in the dataset and then computing the ratio. Print out both values.\n",
    "    - Fill in the null values in the columns 'Calories' and 'TotalSteps' where possible. To fill the values you have to use the factors *\"TotalSteps/Calories\"* and *\"Calories/TotalSteps\"* calculated in the previous point, using one of the two information to fill the other.\n",
    "    - Print out the mean of the columns 'TotalSteps' and 'Calories' before and after filling the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T00:11:46.865951Z",
     "start_time": "2024-03-28T00:11:46.855810Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps/calory: 3.4100446727062272\n",
      "calories/step: 0.29325129022617624\n",
      "Steps mean before filling: 7879.460599334073\n",
      "Calories mean before filling: 2310.661987041037\n",
      "Steps mean after filling: 7785.911198419612\n",
      "Calories mean after filling: 2302.939349901768\n"
     ]
    }
   ],
   "source": [
    "combined_df_unaltered = combined_df.copy()  # make a copy of the data fol later use\n",
    "\n",
    "#calculating the mean of the steps and calories and their ratios\n",
    "steps_mean = combined_df['TotalSteps'].mean()\n",
    "calories_mean = combined_df['Calories'].mean()\n",
    "steps_per_calory = steps_mean / calories_mean\n",
    "calories_per_step = calories_mean / steps_mean\n",
    "print(\"steps/calory: \" + str(steps_per_calory))\n",
    "print(\"calories/step: \" + str(calories_per_step))\n",
    "\n",
    "# filling the gaps in the data where possible, to calculate missing steps, one can take the calories burnt of the same row and multiply it by the average  steps/calory, since calories*(steps/calories) = steps. The same but the other way around for the missing calories.\n",
    "combined_df['TotalSteps'] = combined_df['TotalSteps'].fillna(value=combined_df['Calories'] * steps_per_calory)\n",
    "combined_df['Calories'] = combined_df['Calories'].fillna(value=combined_df['TotalSteps'] * calories_per_step)\n",
    "\n",
    "#printing the old and the new means\n",
    "print('Steps mean before filling: ' + str(steps_mean))\n",
    "print('Calories mean before filling: ' + str(calories_mean))\n",
    "print('Steps mean after filling: ' + str(combined_df['TotalSteps'].mean()))\n",
    "print('Calories mean after filling: ' + str(combined_df['Calories'].mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Dropping missing values** (*2 points*)\n",
    "\n",
    "    - Print how many null values there are in the 'Calories' and 'TotalSteps' columns, respectively.\n",
    "    - Drop the rows where **both** 'Calories' and 'TotalSteps' are missing.\n",
    "    - Print number of rows in the final dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T00:11:46.875334Z",
     "start_time": "2024-03-28T00:11:46.867006Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null values in TotalSteps: 68\n",
      "null values in Calories: 43\n",
      "rows after removing: 956\n"
     ]
    }
   ],
   "source": [
    "combined_df = combined_df_unaltered  # reset combined_df to before the filling.\n",
    "nulls_in_steps: int = combined_df[\n",
    "    'TotalSteps'].isnull().sum()  #is null gives a boolean mask of the null values, since True is 1, if we take the sum we get the number of nulls\n",
    "nulls_in_calories: int = combined_df['Calories'].isnull().sum()\n",
    "print('null values in TotalSteps: ' + str(nulls_in_steps))\n",
    "print('null values in Calories: ' + str(nulls_in_calories))\n",
    "\n",
    "combined_df = combined_df.dropna(subset=['TotalSteps', 'Calories'],\n",
    "                                 how='all')  # only look at the subset of df, since only TotalSteps and Calories are checked, if both are null this is equal to all in the subset are null.\n",
    "\n",
    "rows: int = combined_df.shape[0]\n",
    "print('rows after removing: ' + str(rows))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:left;\">Exercise 3 - Multi-index<span style=\"float: right\">7 points</span></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will create and manipulate a multi-index dataframe. First, let's create the dataframe for the exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T00:11:46.923914Z",
     "start_time": "2024-03-28T00:11:46.917172Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   idx  A_X  A_Y   B_X   B_Y\n",
      "0    0  1.1  1.2  1.11  1.22\n",
      "1    1  1.1  1.2  1.11  1.22\n",
      "2    2  1.1  1.2  1.11  1.22\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"idx\": [0, 1, 2],\n",
    "        \"A_X\": [1.1, 1.1, 1.1],\n",
    "        \"A_Y\": [1.2, 1.2, 1.2],\n",
    "        \"B_X\": [1.11, 1.11, 1.11],\n",
    "        \"B_Y\": [1.22, 1.22, 1.22],\n",
    "    }\n",
    ")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Set the column *idx* as the index of the dataframe. (*1 point*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T00:11:46.952578Z",
     "start_time": "2024-03-28T00:11:46.947344Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A_X  A_Y   B_X   B_Y\n",
      "idx                      \n",
      "0    1.1  1.2  1.11  1.22\n",
      "1    1.1  1.2  1.11  1.22\n",
      "2    1.1  1.2  1.11  1.22\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_reindexed = df.set_index('idx')  # set explicit index \n",
    "print(df_reindexed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a multi-column stucture. (*3 points*)\n",
    "    - Set the columns *A, B* on the first level and *X, Y* on the second level, taken from the combinations in the original dataframe. \n",
    "    - Set the names of the two new levels as \"L1\" and \"L2\", respectively. \n",
    "    - Print the resulting dataframe."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1     A          B      \n",
      "L2     X    Y     X     Y\n",
      "idx                      \n",
      "0    1.1  1.2  1.11  1.22\n",
      "1    1.1  1.2  1.11  1.22\n",
      "2    1.1  1.2  1.11  1.22\n"
     ]
    }
   ],
   "source": [
    "# Define Columns for multi-column structure\n",
    "\n",
    "columns = pd.MultiIndex.from_product([['A', 'B'], ['X', 'Y']], names=['L1', 'L2'])\n",
    "\n",
    "#set new columns to dr_reindexed\n",
    "df_reindexed.columns = columns\n",
    "\n",
    "# print result\n",
    "print(df_reindexed)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T00:11:47.012683Z",
     "start_time": "2024-03-28T00:11:47.005126Z"
    }
   },
   "execution_count": 286
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. From the previous dataframe, re-create a dataframe with a single column level. (*3 points*)\n",
    "    - Create a new column from the first level (L1) of the multi-column. At this point your columns should be ['L1', 'X', 'Y'], with name 'L2'. **NB** The DataFrame method *reset_index* is useful for this part.\n",
    "    - Rename the newly-created column as \"letter\" and the name of the column level as \"L\". Use the appropiate pandas methods for this.\n",
    "    - Print the resulting dataframe."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L   letter     X     Y\n",
      "idx                   \n",
      "0        A  1.10  1.20\n",
      "0        B  1.11  1.22\n",
      "1        A  1.10  1.20\n",
      "1        B  1.11  1.22\n",
      "2        A  1.10  1.20\n",
      "2        B  1.11  1.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lukik\\AppData\\Local\\Temp\\ipykernel_18400\\2735369842.py:2: FutureWarning: The previous implementation of stack is deprecated and will be removed in a future version of pandas. See the What's New notes for pandas 2.1.0 for details. Specify future_stack=True to adopt the new implementation and silence this warning.\n",
      "  df_stacked = df_reindexed.stack(level=0)\n"
     ]
    }
   ],
   "source": [
    "# stack DataFrame, such that L1 is a separate column with A and B as inputs\n",
    "df_stacked = df_reindexed.stack(level=0)\n",
    "\n",
    "# reindex  df_stacked, such that the columns are L1, X, Y and the name is L2\n",
    "\n",
    "df = df_stacked.reset_index(level=1)\n",
    "# rename L1 as letter\n",
    "df = df.rename(columns={\"L1\": \"letter\"})\n",
    "\n",
    "# set L2 to L\n",
    "df = df.rename_axis('L', axis=1)\n",
    "\n",
    "print(df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T00:11:47.062578Z",
     "start_time": "2024-03-28T00:11:47.051497Z"
    }
   },
   "execution_count": 287
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T00:11:47.098334Z",
     "start_time": "2024-03-28T00:11:47.095435Z"
    }
   },
   "execution_count": 287
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dash",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
